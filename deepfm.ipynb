{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Lambda, Add, Dot, Activation, Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.conf_process import myconfigparser\n",
    "from utils.feature_process import tuple_feature_process\n",
    "from utils.active_method import activate\n",
    "from utils.optimizer_method import get_optimizer\n",
    "from utils.distribut_conf import set_dist_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_columns(conf_path):\n",
    "\n",
    "    deep_columns = list()\n",
    "    default_dict = {}\n",
    "    embedding_nums = 0\n",
    "\n",
    "    cfg = myconfigparser()\n",
    "    cfg.read(conf_path)\n",
    "    sections = cfg.sections()\n",
    "\n",
    "    for part_key in sections:\n",
    "\n",
    "        items_lst = cfg.items(part_key)\n",
    "\n",
    "        if part_key == \"deep\":\n",
    "            for items in items_lst:\n",
    "                feature_name = items[0].strip()\n",
    "                feat_method,default_val,embedding_num = tuple_feature_process(items,int(FLAGS.embedding_size))\n",
    "                embedding_nums += embedding_num\n",
    "                if feature_name not in default_dict:\n",
    "                    default_dict[feature_name] = default_val\n",
    "                deep_columns.append(feat_method)\n",
    "\n",
    "    columns = cfg.items(\"use\")[0][1].strip().split(\",\")\n",
    "    default_dict[\"label\"] = 0\n",
    "    cols_default = list(map(lambda x:[\"-1\"] if x not in default_dict else [default_dict[x]], columns))\n",
    "    return deep_columns, columns, cols_default,embedding_nums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixlen_feature_columns = [DenseFeat(feat, 1,) for feat in dense_features] \\\n",
    "                       + [SparseFeat(feat, data[feat].nunique()) for feat in sparse_features\n",
    "                             if data[feat].nunique() < 10000] \\\n",
    "\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "fixlen_feature_names = get_fixlen_feature_names(\n",
    "    linear_feature_columns + dnn_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM(tf.keras.layers.Layer):\n",
    "    \"\"\"Factorization Machine models pairwise (order-2) feature interactions\n",
    "     without linear term and bias.\n",
    "      Input shape\n",
    "        - 3D tensor with shape: ``(batch_size,field_size,embedding_size)``.\n",
    "      Output shape\n",
    "        - 2D tensor with shape: ``(batch_size, 1)``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FM, self).__init__()\n",
    "\n",
    "    def call(self, input):\n",
    "        square_of_sum = tf.math.pow(tf.math.reduce_sum(input, 1, keepdims=True), 2)\n",
    "        sum_of_square = tf.math.reduce_sum(input * input, 1, keepdims=True)\n",
    "        cross_term = square_of_sum - sum_of_square\n",
    "        cross_term = 0.5 * tf.math.reduce_sum(cross_term, axis=2, keepdims=False)\n",
    "\n",
    "        return tf.squeeze(cross_term, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-679faf8e51e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_feature_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdnn_feature_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_emb_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         self.feature_index = build_input_features(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class Base(tf.keras.Model):\n",
    "    def __init__(self, linear_feature_columns, dnn_feature_columns, sparse_emb_dim):\n",
    "        super(Base, self).__init__()\n",
    "\n",
    "        self.feature_index = build_input_features(\n",
    "            linear_feature_columns + dnn_feature_columns)\n",
    "        self.dnn_feature_columns = dnn_feature_columns\n",
    "\n",
    "        self.sparse_feature_columns = list(\n",
    "                filter(lambda x: isinstance(x, SparseFeat), dnn_feature_columns)\n",
    "                                          ) if len(dnn_feature_columns) else []\n",
    "        self.varlen_sparse_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, VarLenSparseFeat), dnn_feature_columns)\n",
    "                                                 ) if dnn_feature_columns else []\n",
    "        self.dense_feature_columns = list(\n",
    "                filter(lambda x: isinstance(x, DenseFeat), dnn_feature_columns)\n",
    "                                         ) if len(dnn_feature_columns) else []\n",
    "\n",
    "        self.embedding_dict = {feat.embedding_name: layers.Embedding(feat.dimension, sparse_emb_dim, embeddings_initializer='normal')\n",
    "                                for feat in self.sparse_feature_columns+self.varlen_sparse_feature_columns}\n",
    "\n",
    "        # self.weight = tf.Variable(tf.random.normal(\n",
    "        #                           [sum(fc.dimension for fc in self.dense_feature_columns), 1],\n",
    "        #                           stddev=0.0001), trainable=True)\n",
    "        self.out_bias= tf.Variable(tf.zeros([1,]), trainable=True)\n",
    "\n",
    "\n",
    "    def input_from_feature_columns(self, x):\n",
    "        sparse_embedding_list = [self.embedding_dict[feat.embedding_name](\n",
    "            x[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]])\n",
    "            for feat in self.sparse_feature_columns]\n",
    "        varlen_sparse_embedding_list = [self.embedding_dict[feat.embedding_name](\n",
    "            x[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]])\n",
    "            for feat in self.varlen_sparse_feature_columns]\n",
    "\n",
    "        dense_value_list = [x[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]] for feat in\n",
    "                            self.dense_feature_columns]\n",
    "\n",
    "        return sparse_embedding_list, \\\n",
    "               varlen_sparse_embedding_list, \\\n",
    "               dense_value_list\n",
    "\n",
    "class DeepFM(Base):\n",
    "    def __init__(self, linear_feature_columns, dnn_feature_columns,\n",
    "                 sparse_emb_dim, dnn_layers, dropout_rate=0.5):\n",
    "        super(DeepFM, self).__init__(linear_feature_columns, dnn_feature_columns,\n",
    "                                     sparse_emb_dim)\n",
    "\n",
    "        self.fm = FM()\n",
    "        self.dnn= tf.keras.Sequential([\n",
    "                            DNN(sum(map(lambda x: x.dimension, self.dense_feature_columns)) +\n",
    "                                  len(self.sparse_feature_columns) * sparse_emb_dim,\n",
    "                                  dnn_layers, dropout_rate=dropout_rate),\n",
    "                            layers.Dense(1, use_bias=False, activation='linear')])\n",
    "\n",
    "    def call(self, x):\n",
    "        sparse_emb, varlen_emb, dense_emb = self.input_from_feature_columns(x)\n",
    "\n",
    "        linear_sparse_logit = tf.reduce_sum(\n",
    "                                tf.concat(sparse_emb, axis=-1), axis=-1, keepdims=False)\n",
    "\n",
    "        if len(dense_emb):\n",
    "            linear_dense_logit = tf.matmul(tf.concat(\n",
    "                                    dense_emb, axis=-1), self.weight)\n",
    "            logit = tf.squeeze(linear_sparse_logit + linear_dense_logit, -1)\n",
    "\n",
    "            logit += self.dnn(tf.concat([tf.squeeze(tf.concat(sparse_emb + varlen_emb, -1), 1),\n",
    "                                        tf.concat(dense_emb, -1)], axis=-1))\n",
    "\n",
    "        else:\n",
    "            logit = tf.squeeze(linear_sparse_logit, -1)\n",
    "\n",
    "            logit += tf.squeeze(self.dnn(tf.concat(\n",
    "                [tf.squeeze(x, 1) for x in sparse_emb] + [tf.reshape(varlen_emb, [x.shape[0],-1])], 1)), 1)\n",
    "\n",
    "            logit += self.fm(tf.concat(sparse_emb + varlen_emb, axis=1))\n",
    "\n",
    "        pred = logit + self.out_bias\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepFM(linear_feature_columns=linear_feature_columns, dnn_feature_columns=dnn_feature_columns, task='binary',\n",
    "               l2_reg_embedding=1e-5, device=device)\n",
    "\n",
    "model.compile(\"adagrad\", \"binary_crossentropy\",\n",
    "              metrics=[\"binary_crossentropy\", \"auc\"],)\n",
    "model.fit(train_model_input, train[target].values,\n",
    "          batch_size=32, epochs=10, validation_split=0.2, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "scale",
   "language": "python",
   "name": "scale"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
